---
layout: mypost
title: PyTorch源码阅读之torch.autograd()
categories: [深度学习tricks]
---
## 1、过拟合和正则化
神经网络是伟大的函数逼近器和特征提取器，但有时它们的权重变得过于特定化，导致过拟合，这就是正则化出现的地方。

正则化可以定义为我们为了减少泛化误差而不是训练误差而对训练算法做出的改变，有许多正规化策略，一个有效的正规化是一个有利可图的交易，能显著减少偏差而不过度增加偏差。

在实践中使用的主要正规化技术有：
- L1正则化
- L2正则化
- 数据增强
- dropout
- early stopping

## 2、L2正则化
在本文中我们重点介绍L2正则化方法，L2正则化属于正则化技术的一类，称为参数范数惩罚项，之所以提到这类技术，是因为在这类技术中，特定参数（主要是权重）被添加到优化的函数中，在L2范数中，网络损失函数中加入一个额外的项，通常称为正则化项。
$$C=C_0 + \frac{\lambda}{2n}\sum_{\omega}\omega^2$$
## 3、L2正则化为什么有用？
神经网络使用反向传播进行梯度更新，对梯度求导后公式为：
$$\frac{\partial C}{\partial \omega} = \frac{\partial C_0}{\partial \omega}+\frac{\lambda}{n}\omega$$
权重变换规则变为：
$$\omega = \omega - \eta\frac{\partial C_0}{\partial \omega} - \frac{\eta \lambda}{n}\omega$$
易见相比于传统的梯度下降法多了一项$- \frac{\eta \lambda}{n}\omega$，当神经网络的一个参数较大时，下降的也比较快，会使整个网络权值相对均衡并总体较小。当对输入做微小扰动时，输出不会剧烈变化，这也有缺点，导致正则化的网络很难学习到数据中的局部噪声，这迫使网络学习到那些在训练集中经常看到的特征。
## 4、L2正则化和权值衰减
L2正则化和权重衰减并不相同，但是可以根据学习率对权值衰减因子进行重新参数化，从而使SGD等价。这部分不是特别详细，以后补上。






**来自知乎文章：https://zhuanlan.zhihu.com/p/342526154**